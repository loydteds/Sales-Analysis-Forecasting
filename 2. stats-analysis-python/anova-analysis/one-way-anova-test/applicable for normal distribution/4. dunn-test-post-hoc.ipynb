{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            group1           group2          p-adj  meandiff  reject  \\\n",
      "0        Furniture        Furniture   1.000000e+00  0.000000   False   \n",
      "1        Furniture  Office Supplies  1.534319e-138  1.481696    True   \n",
      "2        Furniture       Technology   5.207264e-02 -0.201886   False   \n",
      "3  Office Supplies        Furniture  1.534319e-138 -1.481696    True   \n",
      "4  Office Supplies  Office Supplies   1.000000e+00  0.000000   False   \n",
      "5  Office Supplies       Technology  2.001982e-154 -1.683583    True   \n",
      "6       Technology        Furniture   5.207264e-02  0.201886   False   \n",
      "7       Technology  Office Supplies  2.001982e-154  1.683583    True   \n",
      "8       Technology       Technology   1.000000e+00  0.000000   False   \n",
      "\n",
      "      lower     upper  \n",
      "0 -0.127258  0.127258  \n",
      "1  1.376760  1.586633  \n",
      "2 -0.332975 -0.070797  \n",
      "3 -1.586633 -1.376760  \n",
      "4 -0.076346  0.076346  \n",
      "5 -1.793134 -1.574032  \n",
      "6  0.070797  0.332975  \n",
      "7  1.574032  1.793134  \n",
      "8 -0.134812  0.134812  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import plotly.express as px\n",
    "from scipy.stats import levene, bartlett, f_oneway, norm\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:\\\\Users\\\\loydt\\\\Downloads\\\\Projects\\\\Superstore Sales Dataset.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Filter the big four states generating higher sales\n",
    "states_of_interest = ['Washington', 'California', 'New York', 'Florida', 'Pennsylvania']\n",
    "state_data = data[data['State'].isin(states_of_interest)]\n",
    "\n",
    "# Extract relevant columns and create a deep copy to avoid SettingWithCopyWarning\n",
    "high_sales_states = state_data[['Segment', 'State', 'City', 'Region', 'Ship Mode', 'Order Date', 'Category', 'Sub-Category', 'Product Name', 'Sales']].copy()\n",
    "\n",
    "# Take logarithm of the Sales column, ensuring non-positive values are handled\n",
    "high_sales_states['log_sales'] = high_sales_states['Sales'].apply(lambda x: math.log(x) if x > 0 else None)\n",
    "\n",
    "# Check for null values in 'log_sales' after transformation\n",
    "if high_sales_states['log_sales'].isnull().any():\n",
    "    print(\"Warning: There are non-positive sales values that have been transformed to NaN.\")\n",
    "\n",
    "# Create a DataFrame with 'log_sales' and 'Category'\n",
    "dunn_data = high_sales_states[['log_sales', 'Category']].dropna()  # Drop NaN values for valid analysis\n",
    "\n",
    "# Conduct Dunn's Test\n",
    "dunn_results = sp.posthoc_dunn(dunn_data, val_col='log_sales', group_col='Category', p_adjust='bonferroni')\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "dunn_results_df = dunn_results.stack().reset_index()\n",
    "dunn_results_df.columns = ['group1', 'group2', 'p-adj']\n",
    "\n",
    "# Calculate mean difference, lower and upper confidence intervals, and reject\n",
    "dunn_results_df['meandiff'] = dunn_results_df.apply(\n",
    "    lambda row: dunn_data[dunn_data['Category'] == row['group1']]['log_sales'].mean() - \n",
    "                 dunn_data[dunn_data['Category'] == row['group2']]['log_sales'].mean(), axis=1)\n",
    "\n",
    "# Create a 'reject' column based on the adjusted p-value\n",
    "dunn_results_df['reject'] = dunn_results_df['p-adj'] < 0.05\n",
    "\n",
    "# Calculate confidence intervals\n",
    "for index, row in dunn_results_df.iterrows():\n",
    "    group1_data = dunn_data[dunn_data['Category'] == row['group1']]['log_sales']\n",
    "    group2_data = dunn_data[dunn_data['Category'] == row['group2']]['log_sales']\n",
    "    \n",
    "    mean1 = group1_data.mean()\n",
    "    mean2 = group2_data.mean()\n",
    "    \n",
    "    # Calculate standard error\n",
    "    se1 = group1_data.std() / math.sqrt(len(group1_data))\n",
    "    se2 = group2_data.std() / math.sqrt(len(group2_data))\n",
    "    \n",
    "    # Calculate mean difference\n",
    "    meandiff = mean1 - mean2\n",
    "    \n",
    "    # Z-score for 95% confidence interval\n",
    "    z_score = norm.ppf(0.975)  # 1.96 for 95% CI\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    lower_bound = meandiff - z_score * math.sqrt(se1**2 + se2**2)\n",
    "    upper_bound = meandiff + z_score * math.sqrt(se1**2 + se2**2)\n",
    "    \n",
    "    # Assign lower and upper bounds to the DataFrame\n",
    "    dunn_results_df.at[index, 'lower'] = lower_bound\n",
    "    dunn_results_df.at[index, 'upper'] = upper_bound\n",
    "\n",
    "# Display the results\n",
    "print(dunn_results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
